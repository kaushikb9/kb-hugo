+++
title = "Scaling AI, with humans"
slug = "ai-vs-humans-quadrant"
date = 2025-06-21T00:00:00+05:30
description = "A quadrant model to think about where AI should lead, where humans still shine, and when the combo works best."
draft = true
+++

In my org, I am responsible for using AI on two major fronts:

- To solve customer issues at scale  
- To improve org productivity, especially for engineers

Both are exciting and high-leverage areas â€” but the bar, expectations, and constraints differ wildly.

For customers, the tolerance for error is low. We canâ€™t afford to mess things up. Every touchpoint matters â€” understanding their language, reducing back and forth, empathising with their stress. The customer usually reaches out after theyâ€™ve tried everything else. Itâ€™s not enough to be correct â€” we have to help solve their problem.

For internal tooling (especially for engineers), we have more room to experiment and take risks. Developers are usually more forgiving if something fails gracefully, and the payoff for successful automation is massive. Weâ€™re aiming to 10x workflows, not just patch over gaps.

Now, whether itâ€™s external or internal, I started thinking: maybe the issue isnâ€™t just about whether AI is â€œgood enoughâ€. Maybe itâ€™s about understanding *where* AI should lead, where humans still shine, and when the combo works best.

So I drew this quadrant to make sense of the landscape.

![AI vs Humans Quadrant](ai-vs-humans.png)

### Q3: ğŸ§± Crutch Zone  
*Low AI, low human involvement*

These are the problems that shouldnâ€™t really need AI or humans â€” they only exist because of product gaps. This is where humans were in the loop *only* because the product didnâ€™t support something directly. Think rule engines, deterministic workflows, status updates.

These problems donâ€™t need AI. They need product fixes.

- "Resend OTP" â€” should be a button  
- "Why is my email not verified?" â€” validation logic  
- "Send alert if threshold breached" â€” config  

AI sometimes fills this gap, but it's a crutch. It shouldnâ€™t have to.

### Q4: ğŸ¤– The LLM Sweet Spot  
*High AI, minimal human involvement*

These are classic LLM-friendly problems. Thereâ€™s a clear set of data or logic. You can build RAG pipelines, plug into tools, and trust the answer most of the time.

- "Whereâ€™s my order?"  
- "I am not happy - issue a refund"  
- "How do I update billing info?"  
- "What does this dashboard metric mean?"  

Small errors here donâ€™t break the world. The AI gets it right often enough. And even when it doesnâ€™t, users usually forgive. Internally, this is also where a lot of AI-assisted productivity happens â€” suggesting tests, cleaning up stale configs, renaming files, writing boilerplate. These are low-risk, high-reward.

Here are some more examples weâ€™ve seen work well:

- Auto-generating PR descriptions based on code diffs  
- Writing quick integration tests from function signatures  
- Creating GitHub issues or documentation stubs from internal Slack threads  
- Parsing CI/CD logs and summarizing what went wrong  
- Suggesting variable or function names aligned with repo conventions  

The key is: the problem has known structure, AI doesnâ€™t need much context, and mistakes are cheap to fix.

### Q1: ğŸ§  The Combo Zone  
*High AI, high human involvement*

This is the quadrant where AI and humans amplify each other. The combo doesn't just work â€” it compounds. AI does the grunt work, human makes the final call. With the right tools, a human can 10x their impact:

- A support agent becomes capable of resolving deeper technical issues  
- A dev ships faster by offloading boilerplate and tests  
- A smart intern can jump into fixing complex production issues  

These arenâ€™t just "copilots" â€” theyâ€™re amplifiers. The combo wins. Especially internally, this is where the biggest org productivity jumps can come from. But even on the customer side, weâ€™re seeing signs â€” tools that make humans better, not just cheaper.

### Q2: ğŸ‘€ The Judgment Zone  
*High human judgment, low AI reliability*

This quadrant is counterintuitive. Youâ€™d think everything eventually moves toward AI, right? But there are still cases where humans outperform â€” not because the task is complex, but because the *context* or *emotion* isnâ€™t obvious.

- A user has asked a simple question but is frustrated from past unresolved tickets  
- A dispute needs context across teams, people, and old decisions  
- A hotfix needs coordination and judgment under pressure  

Even if the AI gives the right answer, it might escalate things further. The issue isnâ€™t *what* is said, but *how and when* itâ€™s said.

### The hard truth

Most problems feel like they should sit in Q1 or Q4 â€” but end up in Q2 due to missing context. Imagine a customer asking a straightforward question, but their past tickets havenâ€™t been resolved. Theyâ€™ve had to repeat themselves multiple times. The AI answers correctly â€” but it feels like a dead end. If that context were available, the same problem couldâ€™ve easily been handled in one of the other quadrants.

Context can shift a problem across quadrants.

### So what's the verdict?

I think weâ€™ve still got a year or two where humans play a critical role â€” helping make AI agents smarter and faster. Thereâ€™s no doubt weâ€™re heading toward an AI-first future. Itâ€™s better to accept that and start adapting now. I keep asking engineers on my team: *Are you writing code, or are you writing code that can write code?*

This quadrant helped me â€” and my teams â€” decide where to push harder, where to let go, and where to slow down and reframe the problem. Some problems need better prompts. Some need better context. Some just need a human who knows when to say, "I get it. Let's fix this."

Use it to decide whatâ€™s worth automating, whatâ€™s worth fixing, and whatâ€™s still worth doing the old school way.
