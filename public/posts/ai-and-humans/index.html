<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scaling AI, with humans | Kaushik Bhat</title>
<meta name=keywords content><meta name=description content="AI is changing how we work. A mental model that helped me focus on what to focus on, what to let go, and what I need to grow into."><meta name=author content><link rel=canonical href=https://kaushikbhat.com/posts/ai-and-humans/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://kaushikbhat.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://kaushikbhat.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://kaushikbhat.com/favicon-32x32.png><link rel=apple-touch-icon href=https://kaushikbhat.com/apple-touch-icon.png><link rel=mask-icon href=https://kaushikbhat.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://kaushikbhat.com/posts/ai-and-humans/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Scaling AI, with humans"><meta property="og:description" content="AI is changing how we work. A mental model that helped me focus on what to focus on, what to let go, and what I need to grow into."><meta property="og:type" content="article"><meta property="og:url" content="https://kaushikbhat.com/posts/ai-and-humans/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-29T00:00:00+05:30"><meta property="article:modified_time" content="2025-06-29T00:00:00+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scaling AI, with humans"><meta name=twitter:description content="AI is changing how we work. A mental model that helped me focus on what to focus on, what to let go, and what I need to grow into."><meta name=twitter:site content="@kaushikb9"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kaushikbhat.com/posts/"},{"@type":"ListItem","position":2,"name":"Scaling AI, with humans","item":"https://kaushikbhat.com/posts/ai-and-humans/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scaling AI, with humans","name":"Scaling AI, with humans","description":"AI is changing how we work. A mental model that helped me focus on what to focus on, what to let go, and what I need to grow into.","keywords":[],"articleBody":"AI is everywhere ‚Äî but figuring out where it actually helps (and where it doesn‚Äôt) is still very much a work in progress. In my current role, I lead 2 charters - tooling for customer success and improve engineering productivity. Over the last few months, we have been carefully exploring how AI can play a meaningful role across both.\nWhen automating Customer support, the tolerance for error is low. We can‚Äôt afford to mess things up. Every touchpoint matters - understanding their language, reducing back and forth, empathising with their frustrations. The customer usually reaches out after they‚Äôve tried everything else. It‚Äôs not enough to be correct - the customer needs to feel that their concern is actually being addressed.\nOn the other side, when building for internal users, especially engineering, we have more room to experiment and can take some risks. Developers are usually more forgiving if something fails gracefully. We want to optimise for speed and cost - not just build some gimmicky features.\nSome experiments worked, some didn‚Äôt. Along the way, I started building a mental model for where AI can really shine, where humans still matter, and where the combo works best. So I drew this quadrant to make sense of the landscape.\nQ3: üß± Crutch Zone Low AI, low human involvement\nThese are the problems that shouldn‚Äôt really need AI or humans. They only exist because of product gaps. This is where humans exist in the loop only because the product didn‚Äôt support something directly. Think rule engines, deterministic workflows, status updates. These problems don‚Äôt need AI. They need product fixes.\n‚ÄúCheck eligibility‚Äù - should be a button on the dashboard ‚ÄúWhy is my email not verified?‚Äù - validation logic ‚ÄúSend alert if threshold breached‚Äù - runbook config AI sometimes fills this gap, but it‚Äôs a crutch. It shouldn‚Äôt have to.\nQ4: ü§ñ The LLM Sweet Spot High AI, minimal human involvement\nThese are classic LLM-friendly problems. There is a clear set of data or logic. You can build RAG pipelines, plug into tools, and trust the answer most of the time.\n‚ÄúWhy is my refund taking so long?‚Äù ‚ÄúI am unable to understand the documentation‚Äù ‚ÄúWhat does this dashboard metric mean?‚Äù The AI gets it right often enough but sometimes it can afford to do small mistakes. These are low-risk, high-reward. Here are some more examples that can do well for improving dev experience:\nAuto-generating PR descriptions based on code diffs Writing quick integration tests from function signatures Creating GitHub issues or documentation stubs from internal Slack threads Parsing CI/CD logs and summarizing what went wrong The key is: the problem has known structure, AI doesn‚Äôt need much context, and mistakes are cheap to fix.\nQ1: üß† The Combo Zone High AI, high human involvement\nThis is the quadrant where AI and humans amplify each other. The combo has a compounding effect. AI does the grunt work, human makes the final call. With the right tools, a smart human can 10x their impact:\nA support agent becomes capable of resolving deeper technical issues A dev can focus on design decisions and edge cases by offloading boilerplate and tests A fresher can jump into fixing complex production issues In many ways, LLMs feel like that one super-enthu junior engineer - smart and fast, but needs direction. They can draft, summarize, connect dots, and suggest next steps but could sometimes make mistakes. When paired with experienced humans, the combo is unbeatable, else they can drift easily.\nQ2: üëÄ The Judgment Zone High human judgment, low AI reliability\nThis quadrant is counterintuitive. You‚Äôd think everything eventually moves towards AI, right? But there are still cases where humans can outperform AI - not because the task is complex, but because the context or emotion isn‚Äôt obvious.\nA customer has asked a simple question but is frustrated because there are other tickets which are still open A dispute needs context across teams, people, and old decisions A hotfix needs coordination and judgment under pressure Even if AI can give the right answer, it might escalate things further. The issue isn‚Äôt what is said, but how and when it‚Äôs said.\nThe hard truth Most problems feel like they should sit in the 1st, 3rd or the 4th quadrant - but they often end up in the 2nd quadrant aka The Judgement Zone due to missing context. Imagine a customer asking a straightforward question but the AI gives a templated answer or asks for more details like a screenshot or logs or worse - replicate the scenario. The customer is already frustrated and now they have to do more tasks to get their issue resolve. If that context was pre-fetched, the same problem could have easily been handled in one of the other quadrants.\nWhile I was writing this post, a talk by Andrej Karpathy was doing the rounds. It reinforced a few things we have been seeing on the ground, especially around the role of AI agents and the importance of context. He talks about AI agents as the next layer of abstraction and how chaining tools and memory into them is what makes them powerful. He also makes a great point about designing for LLMs, not just using them as a drop-in replacement for humans. In other words: don‚Äôt just give the model a messy interface or complex tool and expect magic - build clean, clear scaffolds around the model‚Äôs strengths. That resonates with our experience too. Problems in the LLM Sweet Spot work well because we‚Äôve shaped the inputs and constraints to help the model succeed. He also talks about human-in-the-loop where we need to design systems where users can verify, edit, and guide the AI.\nEvolving with AI We haven‚Äôt cracked all of this. Some parts are still rough. But thinking in this way has helped us ask better questions and build towards stronger answers. This model helped me pause and rethink how I should be strategising to leverage AI better. There is no doubt we are heading toward an AI-first future, but I think we still have a year or two where humans play a critical role ‚Äî helping make AI agents smarter, faster, and more useful. It‚Äôs better to accept that and start upskilling ourselves for an exciting future.\nI keep asking engineers on my team: ‚ÄúAre you writing code, or are you writing code that can write code?‚Äù That shift means rethinking how we structure inputs, build feedback interfaces, store memory, and even write docs.\nAs Andrej said in his talk: ‚ÄúYou don‚Äôt want to use LLMs, you want to build for them.‚Äù\n","wordCount":"1099","inLanguage":"en","datePublished":"2025-06-29T00:00:00+05:30","dateModified":"2025-06-29T00:00:00+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"https://kaushikbhat.com/posts/ai-and-humans/"},"publisher":{"@type":"Organization","name":"Kaushik Bhat","logo":{"@type":"ImageObject","url":"https://kaushikbhat.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kaushikbhat.com/ accesskey=h title="Kaushik Bhat (Alt + H)">Kaushik Bhat</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kaushikbhat.com/about title=About><span>About</span></a></li><li><a href=https://kaushikbhat.com/blog title=Blog><span>Blog</span></a></li><li><a href=https://kaushikbhat.com/notes title=Notes><span>Notes</span></a></li><li><a href=https://kaushikbhat.com/ideas title=Ideas><span>Ideas</span></a></li><li><a href=https://kaushikbhat.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Scaling AI, with humans</h1><div class=post-description>AI is changing how we work. A mental model that helped me focus on what to focus on, what to let go, and what I need to grow into.</div><div class=post-meta><span title='2025-06-29 00:00:00 +0530 IST'>June 29, 2025</span>&nbsp;¬∑&nbsp;6 min</div></header><div class=post-content><p>AI is everywhere ‚Äî but figuring out where it actually helps (and where it doesn‚Äôt) is still very much a work in progress. In my current role, I lead 2 charters - tooling for customer success and improve engineering productivity. Over the last few months, we have been carefully exploring how AI can play a meaningful role across both.</p><p>When automating Customer support, the tolerance for error is low. We can‚Äôt afford to mess things up. Every touchpoint matters - understanding their language, reducing back and forth, empathising with their frustrations. The customer usually reaches out after they‚Äôve tried everything else. It‚Äôs not enough to be correct - the customer needs to feel that their concern is actually being addressed.</p><p>On the other side, when building for internal users, especially engineering, we have more room to experiment and can take some risks. Developers are usually more forgiving if something fails gracefully. We want to optimise for speed and cost - not just build some gimmicky features.</p><p>Some experiments worked, some didn‚Äôt. Along the way, I started building a mental model for where AI can really shine, where humans still matter, and where the combo works best. So I drew this quadrant to make sense of the landscape.</p><p><img loading=lazy src=ai-and-humans.png alt="AI vs Humans Quadrant"></p><h3 id=q3--crutch-zone>Q3: üß± Crutch Zone<a hidden class=anchor aria-hidden=true href=#q3--crutch-zone>#</a></h3><p><em>Low AI, low human involvement</em></p><p>These are the problems that shouldn‚Äôt really need AI or humans. They only exist because of product gaps. This is where humans exist in the loop only because the product didn‚Äôt support something directly. Think rule engines, deterministic workflows, status updates. These problems don‚Äôt need AI. They need product fixes.</p><ul><li>&ldquo;Check eligibility&rdquo; - should be a button on the dashboard</li><li>&ldquo;Why is my email not verified?&rdquo; - validation logic</li><li>&ldquo;Send alert if threshold breached&rdquo; - runbook config</li></ul><p>AI sometimes fills this gap, but it&rsquo;s a crutch. It shouldn‚Äôt have to.</p><h3 id=q4--the-llm-sweet-spot>Q4: ü§ñ The LLM Sweet Spot<a hidden class=anchor aria-hidden=true href=#q4--the-llm-sweet-spot>#</a></h3><p><em>High AI, minimal human involvement</em></p><p>These are classic LLM-friendly problems. There is a clear set of data or logic. You can build RAG pipelines, plug into tools, and trust the answer most of the time.</p><ul><li>&ldquo;Why is my refund taking so long?&rdquo;</li><li>&ldquo;I am unable to understand the documentation&rdquo;</li><li>&ldquo;What does this dashboard metric mean?&rdquo;</li></ul><p>The AI gets it right often enough but sometimes it can afford to do small mistakes. These are low-risk, high-reward. Here are some more examples that can do well for improving dev experience:</p><ul><li>Auto-generating PR descriptions based on code diffs</li><li>Writing quick integration tests from function signatures</li><li>Creating GitHub issues or documentation stubs from internal Slack threads</li><li>Parsing CI/CD logs and summarizing what went wrong</li></ul><p>The key is: the problem has known structure, AI doesn‚Äôt need much context, and mistakes are cheap to fix.</p><h3 id=q1--the-combo-zone>Q1: üß† The Combo Zone<a hidden class=anchor aria-hidden=true href=#q1--the-combo-zone>#</a></h3><p><em>High AI, high human involvement</em></p><p>This is the quadrant where AI and humans amplify each other. The combo has a compounding effect. AI does the grunt work, human makes the final call. With the right tools, a smart human can 10x their impact:</p><ul><li>A support agent becomes capable of resolving deeper technical issues</li><li>A dev can focus on design decisions and edge cases by offloading boilerplate and tests</li><li>A fresher can jump into fixing complex production issues</li></ul><p>In many ways, LLMs feel like that one super-enthu junior engineer - smart and fast, but needs direction. They can draft, summarize, connect dots, and suggest next steps but could sometimes make mistakes. When paired with experienced humans, the combo is unbeatable, else they can drift easily.</p><h3 id=q2--the-judgment-zone>Q2: üëÄ The Judgment Zone<a hidden class=anchor aria-hidden=true href=#q2--the-judgment-zone>#</a></h3><p><em>High human judgment, low AI reliability</em></p><p>This quadrant is counterintuitive. You‚Äôd think everything eventually moves towards AI, right? But there are still cases where humans can outperform AI - not because the task is complex, but because the context or emotion isn‚Äôt obvious.</p><ul><li>A customer has asked a simple question but is frustrated because there are other tickets which are still open</li><li>A dispute needs context across teams, people, and old decisions</li><li>A hotfix needs coordination and judgment under pressure</li></ul><p>Even if AI can give the right answer, it might escalate things further. The issue isn‚Äôt what is said, but how and when it‚Äôs said.</p><h3 id=the-hard-truth>The hard truth<a hidden class=anchor aria-hidden=true href=#the-hard-truth>#</a></h3><p>Most problems feel like they should sit in the 1st, 3rd or the 4th quadrant - but they often end up in the 2nd quadrant aka <strong>The Judgement Zone</strong> due to missing context. Imagine a customer asking a straightforward question but the AI gives a templated answer or asks for more details like a screenshot or logs or worse - <em>replicate the scenario</em>. The customer is already frustrated and now they have to do more tasks to get their issue resolve. If that context was pre-fetched, the same problem could have easily been handled in one of the other quadrants.</p><p>While I was writing this post, a talk <a href="https://youtu.be/LCEmiRjPEtQ?si=e_TVa6I6WZVOlrru">by Andrej Karpathy</a> was doing the rounds. It reinforced a few things we have been seeing on the ground, especially around the role of AI agents and the importance of context. He talks about AI agents as the next layer of abstraction and how chaining tools and memory into them is what makes them powerful. He also makes a great point about designing for LLMs, not just using them as a drop-in replacement for humans. In other words: don‚Äôt just give the model a messy interface or complex tool and expect magic - build clean, clear scaffolds around the model‚Äôs strengths. That resonates with our experience too. Problems in the <strong>LLM Sweet Spot</strong> work well because we‚Äôve shaped the inputs and constraints to help the model succeed. He also talks about human-in-the-loop where we need to design systems where users can verify, edit, and guide the AI.</p><h3 id=evolving-with-ai>Evolving with AI<a hidden class=anchor aria-hidden=true href=#evolving-with-ai>#</a></h3><p>We haven‚Äôt cracked all of this. Some parts are still rough. But thinking in this way has helped us ask better questions and build towards stronger answers. This model helped me pause and rethink how I should be strategising to leverage AI better. There is no doubt we are heading toward an AI-first future, but I think we still have a year or two where humans play a critical role ‚Äî helping make AI agents smarter, faster, and more useful. It‚Äôs better to accept that and start upskilling ourselves for an exciting future.</p><p>I keep asking engineers on my team: &ldquo;Are you writing code, or are you writing code that can write code?&rdquo; That shift means rethinking how we structure inputs, build feedback interfaces, store memory, and even write docs.</p><p>As Andrej said in his talk: ‚ÄúYou don‚Äôt want to use LLMs, you want to build for them.‚Äù</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://kaushikbhat.com/>Kaushik Bhat</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>